{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94c4c0ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f05209d13a4710928eab530ce415f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa32f5ac61e44ca481bf5031fcd6fa01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fec1ddab9424747bfe9b3f7c1c9e13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6faf23e33044ae88ffbeff454eb1a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab58e5fdd8d494199070b3fafcf6e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Summary saved to data/groq_summary.txt\n",
      "\n",
      "Summary Preview:\n",
      " Bert Mueller, a 35-year-old American, founded California Burrito, a successful Mexican-inspired restaurant chain in India.  He overcame challenges like sourcing ingredients and adapting flavors to the local palate by investing in his own avocado farm and building a strong supply chain.  With 103 stores and $23 million in revenue, the company aims to expand to 300 stores by 2030 and potentially go public.  Mueller embraces the challenges and rewards of living and working in India, finding the cul...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.environ[\"GROQ_API_KEY\"]\n",
    "\n",
    "# 1. Load your cleaned transcript\n",
    "with open(\"data/cleaned_transcript.txt\", \"r\") as f:\n",
    "    cleaned_text = f.read()\n",
    "\n",
    "# 2. Configure Groq (uses Mixtral 8x7b by default)\n",
    "groq_llm = ChatGroq(\n",
    "    api_key = api_key,\n",
    "    temperature=0.2,  # Lower = more factual\n",
    "    model_name=\"gemma2-9b-it\",  # Fastest model\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "# 3. Split text into manageable chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=4000,  # Optimal for Groq's context window\n",
    "    chunk_overlap=200\n",
    ")\n",
    "texts = text_splitter.create_documents([cleaned_text])\n",
    "\n",
    "# 4. Summarize using map-reduce (best for long docs)\n",
    "chain = load_summarize_chain(\n",
    "    groq_llm,\n",
    "    chain_type=\"map_reduce\",  # Alternatives: \"stuff\" (short docs), \"refine\" (highest quality)\n",
    "    verbose=False  # Set to True to see intermediate steps\n",
    ")\n",
    "summary = chain.run(texts)\n",
    "\n",
    "# 5. Save the summary\n",
    "with open(\"data/groq_summary.txt\", \"w\") as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"✅ Summary saved to data/groq_summary.txt\")\n",
    "print(\"\\nSummary Preview:\\n\", summary[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ca17709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities:\n",
      "\n",
      "Bert Mueller (PERSON)\n",
      "35-year-old (DATE)\n",
      "American (NORP)\n",
      "California Burrito (ORG)\n",
      "Mexican (NORP)\n",
      "India (GPE)\n",
      "103 (CARDINAL)\n",
      "$23 million (MONEY)\n",
      "300 (CARDINAL)\n",
      "2030 (DATE)\n",
      "Mueller (PERSON)\n",
      "India (GPE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/spacy/displacy/__init__.py:106: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  warnings.warn(Warnings.W011)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bert Mueller\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", a \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    35-year-old\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    American\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       ", founded \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    California Burrito\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", a successful \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mexican\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       "-inspired restaurant chain in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    India\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".  He overcame challenges like sourcing ingredients and adapting flavors to the local palate by investing in his own avocado farm and building a strong supply chain.  With \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    103\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " stores and \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $23 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " in revenue, the company aims to expand to \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    300\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " stores by \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2030\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " and potentially go public.  \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mueller\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " embraces the challenges and rewards of living and working in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    India\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", finding the culture and food incredibly enriching.  <br></div>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5005 ...\n",
      "\n",
      "Shutting down server on port 5005.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Load the English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # You can also use en_core_web_lg for better accuracy\n",
    "\n",
    "# Read the summarized text from a file (you can also replace this with your own string)\n",
    "with open(\"data/groq_summary.txt\", \"r\") as f:\n",
    "    summary_text = f.read()\n",
    "\n",
    "# Apply the NLP pipeline\n",
    "doc = nlp(summary_text)\n",
    "\n",
    "# Print the named entities\n",
    "print(\"Named Entities:\\n\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} ({ent.label_})\")\n",
    "\n",
    "# Optional: visualize entities in browser\n",
    "displacy.serve(doc, style=\"ent\",port = 5005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6fb68974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ transcript_data.json saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# --- Required Inputs ---\n",
    "# Replace these with your actual variables\n",
    "full_transcript = open(\"data/cleaned_transcript.txt\", \"r\").read()\n",
    "summary = open(\"data/groq_summary.txt\", \"r\").read()  # optional, if you've saved it\n",
    "chunk_size = 5\n",
    "\n",
    "# Split and create chunks\n",
    "sentences = full_transcript.strip().split('.')\n",
    "transcript_chunks = [\n",
    "    {\n",
    "        \"chunk_id\": i + 1,\n",
    "        \"text\": '. '.join(sentences[i:i + chunk_size]).strip()\n",
    "    }\n",
    "    for i in range(0, len(sentences), chunk_size)\n",
    "]\n",
    "\n",
    "# Extract named entities\n",
    "doc = nlp(full_transcript)\n",
    "entities = [{\"text\": ent.text, \"label\": ent.label_} for ent in doc.ents]\n",
    "\n",
    "# Create JSON structure\n",
    "transcript_json = {\n",
    "    \"transcript_id\": \"california_burrito_story\",\n",
    "    \"summary\": summary.strip(),\n",
    "    \"entities\": entities,\n",
    "    \"transcript_chunks\": transcript_chunks\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "with open(\"data/transcript_data.json\", \"w\") as f:\n",
    "    json.dump(transcript_json, f, indent=4)\n",
    "\n",
    "print(\"✅ transcript_data.json saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebaa7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the data is large then i would use json format in faiss otherwise not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d75e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/guranshchugh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \n",
      "started Mexican restaurants\n",
      "Distance: 0.9324820041656494\n",
      "\n",
      "Sentence: \n",
      "cuisine to India\n",
      "Distance: 1.0712473392486572\n",
      "\n",
      "Sentence:   when I looked at starting a Mexican-inspired restaurant in India, there was just Taco Bell\n",
      "Distance: 1.1184529066085815\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import faiss\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Download punkt tokenizer from nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the model for sentence embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Or use any other model\n",
    "\n",
    "# Load transcript\n",
    "with open(\"data/cleaned_transcript.txt\", \"r\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# 2. Convert sentences to embeddings\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# 3. Convert embeddings to numpy array\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "# 4. Create the FAISS index\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])  # L2 distance for similarity search\n",
    "index.add(embeddings)  # Add embeddings to the index\n",
    "\n",
    "# Save the FAISS index (optional, to reload it later)\n",
    "faiss.write_index(index, 'data/embeddings/faiss_index.index')\n",
    "\n",
    "# 5. Perform a Search\n",
    "def search(query, top_k=3):\n",
    "    # Convert the query to embeddings\n",
    "    query_embedding = model.encode([query])\n",
    "\n",
    "    # Search the FAISS index for the top_k most similar sentences\n",
    "    distances, indices = index.search(np.array(query_embedding), top_k)\n",
    "\n",
    "    # Retrieve and print the top_k most similar sentences\n",
    "    results = []\n",
    "    for i in range(top_k):\n",
    "        result = {\n",
    "            \"sentence\": sentences[indices[0][i]],  # Get sentence by index\n",
    "            \"distance\": distances[0][i]  # Get distance from query\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example search query\n",
    "query = \"Tell me about the restaurant journey\"\n",
    "search_results = search(query)\n",
    "\n",
    "# Print search results\n",
    "for result in search_results:\n",
    "    print(f\"Sentence: {result['sentence']}\\nDistance: {result['distance']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb73957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462da02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
