{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e80f141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradio 3.41.2 requires aiofiles<24.0,>=22.0, but you have aiofiles 24.1.0 which is incompatible.\n",
      "gradio 3.41.2 requires markupsafe~=2.0, but you have markupsafe 3.0.2 which is incompatible.\n",
      "gradio 3.41.2 requires websockets<12.0,>=10.0, but you have websockets 15.0.1 which is incompatible.\n",
      "gradio-client 0.5.0 requires websockets<12.0,>=10.0, but you have websockets 15.0.1 which is incompatible.\n",
      "langflow 1.1.4 requires certifi<2025.0.0,>=2023.11.17, but you have certifi 2025.1.31 which is incompatible.\n",
      "langflow 1.1.4 requires certifi==2024.8.30, but you have certifi 2025.1.31 which is incompatible.\n",
      "langflow 1.1.4 requires langchain==0.3.10, but you have langchain 0.3.17 which is incompatible.\n",
      "langflow 1.1.4 requires langchain-openai==0.2.12, but you have langchain-openai 0.3.3 which is incompatible.\n",
      "realtime 1.0.6 requires websockets<13,>=11, but you have websockets 15.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "open-clip-torch 2.20.0 requires protobuf<4, but you have protobuf 5.29.4 which is incompatible.\n",
      "unstructured-inference 0.8.6 requires pdfminer-six==20240706, but you have pdfminer-six 20231228 which is incompatible.\n",
      "vllm 0.6.3.post1 requires torch==2.4.0, but you have torch 2.7.0 which is incompatible.\n",
      "vllm 0.6.3.post1 requires torchvision==0.19, but you have torchvision 0.22.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mUsing device: cpu\n",
      "[youtube] Extracting URL: https://youtu.be/3enHvs7VaN8\n",
      "[youtube] 3enHvs7VaN8: Downloading webpage\n",
      "[youtube] 3enHvs7VaN8: Downloading ios player API JSON\n",
      "[youtube] 3enHvs7VaN8: Downloading mweb player API JSON\n",
      "[youtube] 3enHvs7VaN8: Downloading player 8102da6c\n",
      "\u001b[0;33mWARNING:\u001b[0m [youtube] Falling back to generic n function search\n",
      "         player = https://www.youtube.com/s/player/8102da6c/player_ias.vflset/en_US/base.js\n",
      "\u001b[0;33mWARNING:\u001b[0m [youtube] 3enHvs7VaN8: nsig extraction failed: Some formats may be missing\n",
      "         n = B2XPHlPh_rhKm2gJ ; player = https://www.youtube.com/s/player/8102da6c/player_ias.vflset/en_US/base.js\n",
      "\u001b[0;33mWARNING:\u001b[0m [youtube] Falling back to generic n function search\n",
      "         player = https://www.youtube.com/s/player/8102da6c/player_ias.vflset/en_US/base.js\n",
      "\u001b[0;33mWARNING:\u001b[0m [youtube] 3enHvs7VaN8: nsig extraction failed: Some formats may be missing\n",
      "         n = -CS0H83UYTS-mzZb ; player = https://www.youtube.com/s/player/8102da6c/player_ias.vflset/en_US/base.js\n",
      "[youtube] 3enHvs7VaN8: Downloading m3u8 information\n",
      "[info] 3enHvs7VaN8: Downloading 1 format(s): 251\n",
      "[download] Destination: downloaded_audio.webm\n",
      "\u001b[K[download] 100% of    6.28MiB in \u001b[1;37m00:00:00\u001b[0m at \u001b[0;32m13.64MiB/s\u001b[0m;33m00:00\u001b[0m\n",
      "[ExtractAudio] Destination: downloaded_audio.wav\n",
      "Deleting original file downloaded_audio.webm (pass -k to keep)\n",
      "Downloaded and converted to WAV: downloaded_audio.wav\n"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "!pip install -q yt-dlp ffmpeg-python transformers librosa noisereduce accelerate\n",
    "!pip install -q --upgrade torch torchvision torchaudio\n",
    "# Imports\n",
    "import os\n",
    "import librosa\n",
    "import torch\n",
    "import noisereduce as nr\n",
    "import ffmpeg\n",
    "# from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. Download YouTube Audio using yt_dlp\n",
    "video_url = input(\"Enter YouTube Video URL: \")\n",
    "\n",
    "# Download audio in best quality\n",
    "!yt-dlp -f bestaudio --extract-audio --audio-format wav --audio-quality 0 -o \"downloaded_audio.wav\" \"{video_url}\"\n",
    "downloaded_audio_path = \"downloaded_audio.wav\"\n",
    "print(f\"Downloaded and converted to WAV: {downloaded_audio_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b100d92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denoised audio saved at denoised_audio.wav\n"
     ]
    }
   ],
   "source": [
    "# 2. Denoising\n",
    "import soundfile as sf\n",
    "def denoise_audio(audio_path):\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    reduced_noise = nr.reduce_noise(y=y, sr=sr)\n",
    "    denoised_path = 'denoised_audio.wav'\n",
    "    # Use soundfile.write to save the audio instead of librosa.output.write_wav\n",
    "    sf.write(denoised_path, reduced_noise, sr)  \n",
    "    return denoised_path, sr\n",
    "  \n",
    "denoised_audio_path, sample_rate = denoise_audio(downloaded_audio_path)\n",
    "print(f\"Denoised audio saved at {denoised_audio_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec8ecf33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 16 chunks for transcription.\n"
     ]
    }
   ],
   "source": [
    "# 3. Chunking\n",
    "import soundfile as sf # import soundfile\n",
    "def chunk_audio(audio_path, chunk_length_sec=30):\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    total_duration = librosa.get_duration(y=y, sr=sr)\n",
    "    chunks = []\n",
    "\n",
    "    for start in range(0, int(total_duration), chunk_length_sec):\n",
    "        end = min(start + chunk_length_sec, total_duration)\n",
    "        chunk = y[int(start * sr):int(end * sr)]\n",
    "        chunk_path = f'chunk_{start}_{end}.wav'\n",
    "        # Use soundfile to write the audio instead\n",
    "        sf.write(chunk_path, chunk, sr)  \n",
    "        chunks.append(chunk_path)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_audio(denoised_audio_path)\n",
    "print(f\"Created {len(chunks)} chunks for transcription.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1502c14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai-whisper in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (20240930)\n",
      "Requirement already satisfied: more-itertools in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai-whisper) (10.6.0)\n",
      "Requirement already satisfied: numba in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai-whisper) (0.59.1)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai-whisper) (1.26.4)\n",
      "Requirement already satisfied: tiktoken in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai-whisper) (0.7.0)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai-whisper) (2.7.0)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai-whisper) (4.67.1)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from numba->openai-whisper) (0.42.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tiktoken->openai-whisper) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.1.31)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->openai-whisper) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->openai-whisper) (4.13.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->openai-whisper) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->openai-whisper) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->openai-whisper) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->openai-whisper) (2025.3.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch->openai-whisper) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --no-cache-dir openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badb8448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00.000 --> 00:06.880]  Coming to India to do business is a big learning curve.\n",
      "[00:06.880 --> 00:12.160]  First India is a learning curve, then business in India is a further learning curve.\n",
      "[00:12.160 --> 00:17.120]  For me, it was all about letting India be India.\n",
      "[00:17.120 --> 00:24.960]  If you find monotony dull, if you find comfort boring, then India is a perfect place to be.\n",
      "[00:24.960 --> 00:27.640]  Nothing is predictable, every day is different.\n",
      "[00:27.640 --> 00:29.880]  I took it as part of the course that...\n",
      "✅ Successfully transcribed chunk_0_30.wav\n",
      "[00:00.000 --> 00:03.440]  I would just have to adjust to this in order to be successful here.\n",
      "[00:11.680 --> 00:14.320]  My name is Bert Mueller. I'm 35 years old.\n",
      "[00:14.320 --> 00:18.720]  I live in Bengaluru, India and I moved here in December 2011.\n",
      "[00:20.720 --> 00:26.240]  So when I looked at starting a Mexican-inspired restaurant in India, there was just Taco Bell.\n",
      "[00:26.240 --> 00:30.000]  Even now, 13 years in the future, very few people have seen it.\n",
      "✅ Successfully transcribed chunk_30_60.wav\n",
      "[00:00.000 --> 00:05.760]  started Mexican restaurants. Today with California Burrito we have 103 stores.\n",
      "[00:06.560 --> 00:13.920]  This journey from one to 103 stores has taken us almost 12 plus years. When we started our first\n",
      "[00:13.920 --> 00:23.600]  store in our first financial year we did about half a million dollars. Last year we did 23\n",
      "[00:23.600 --> 00:30.000]  million dollars instead of about 85 rupees to the dollar.\n",
      "✅ Successfully transcribed chunk_60_90.wav\n",
      "[00:00.000 --> 00:08.000]  Starting out in India was very challenging, but I never doubted that I made the right decision.\n",
      "[00:08.000 --> 00:14.000]  And I think India's scratched me as a person, and it's really a wonderful place to be. I'm very happy living here.\n",
      "[00:17.000 --> 00:23.000]  The apartment I stay in has three bedrooms, it's a duplex, and it cost me about $1,200 a month.\n",
      "[00:23.000 --> 00:28.000]  And a lot of these furnitures also were my grandparents, so it's passed through the family,\n",
      "[00:28.000 --> 00:30.000]  and now it really feels like my old house.\n",
      "✅ Successfully transcribed chunk_90_120.wav\n",
      "[00:00.000 --> 00:07.920]  in the U.S. here in India. This is where I take my coffee in the morning and look out over the\n",
      "[00:07.920 --> 00:17.360]  small garden I have. This is my small garden, my patio. I have a large avocado tree right over here.\n",
      "[00:17.360 --> 00:20.960]  Now it's not usually raining in Bangalore right now, this is very unseasonable.\n",
      "[00:24.160 --> 00:27.440]  I grew up in Silver Spring, Maryland right outside Washington, D.C.\n",
      "[00:27.440 --> 00:29.840]  Very traditional suburban middle\n",
      "✅ Successfully transcribed chunk_120_150.wav\n",
      "[00:00.000 --> 00:06.740]  class American childhood I would say. I went to the College of William & Mary in Virginia.\n",
      "[00:06.740 --> 00:15.120]  I had a double major in music and public policy. I had the opportunity to study abroad and\n",
      "[00:15.120 --> 00:19.980]  I saw most people were going to Spain or Italy and I'm a bit of a contrarian and I wanted\n",
      "[00:19.980 --> 00:26.580]  to go somewhere that was radically different than the U.S. So I decided that India was\n",
      "[00:26.580 --> 00:27.340]  the place to be.\n",
      "✅ Successfully transcribed chunk_150_180.wav\n",
      "[00:00.000 --> 00:05.500]  Gosh, my first trip to India was just crazy in a sense.\n",
      "[00:05.500 --> 00:08.240]  It was so different than what I had grown up with.\n",
      "[00:08.240 --> 00:14.320]  I came out here with about 20 people on the study abroad program.\n",
      "[00:14.600 --> 00:15.600]  Some people got sick.\n",
      "[00:15.600 --> 00:17.320]  Some people, you know, didn't like the food.\n",
      "[00:17.320 --> 00:18.660]  It didn't treat their system well.\n",
      "[00:20.180 --> 00:24.160]  I was certainly someone who blossomed and was very comfortable navigating India,\n",
      "[00:24.160 --> 00:26.160]  whereas most other people in the program weren't.\n",
      "[00:26.160 --> 00:30.000]  As an individual.\n",
      "✅ Successfully transcribed chunk_180_210.wav\n",
      "[00:00.000 --> 00:02.480]  I've always been somebody who's extremely frugal.\n",
      "[00:04.400 --> 00:07.880]  What do I spend in terms of groceries?\n",
      "[00:07.900 --> 00:10.880]  I spend anywhere between 150 to $200 a month.\n",
      "[00:13.640 --> 00:16.560]  Once I came to India and then I went back,\n",
      "[00:16.600 --> 00:18.920]  I never wanted to have Indian food in the U.S. again,\n",
      "[00:19.320 --> 00:21.560]  because whatever I had in India was so good,\n",
      "[00:21.560 --> 00:23.600]  everything else paled in comparison.\n",
      "[00:25.240 --> 00:27.800]  On eating out, it's probably another $200 a month,\n",
      "[00:27.800 --> 00:29.960]  maybe $400 a month.\n",
      "✅ Successfully transcribed chunk_210_240.wav\n",
      "[00:00.000 --> 00:01.460]  It's a very active month.\n",
      "[00:03.400 --> 00:07.740]  The challenges of living here is it's, it's, it can be frustrating to get things done.\n",
      "[00:08.180 --> 00:10.820]  Patience is kind of the right strategy.\n",
      "[00:10.820 --> 00:13.780]  Usually, you know, they say fortune favors the bold.\n",
      "[00:14.520 --> 00:16.300]  I would say India favors the patient.\n",
      "[00:16.900 --> 00:21.200]  The other thing is, you know, the knock on wood, I had actually not had any health issues.\n",
      "[00:21.320 --> 00:26.260]  Uh, but last year I got dengue in, uh, in August and then I got typhoid in January.\n",
      "[00:26.300 --> 00:28.220]  So I've gone for 13 years with no problems.\n",
      "[00:28.220 --> 00:29.500]  And then last year I got two.\n",
      "✅ Successfully transcribed chunk_240_270.wav\n",
      "[00:00.000 --> 00:04.140]  So, you know, there are health issues that can arise that would not happen in the U.S.\n",
      "[00:04.140 --> 00:07.100]  and these are not small issues.\n",
      "[00:10.340 --> 00:12.640]  All the people in our program were living with families.\n",
      "[00:12.980 --> 00:15.360]  One of my friends was of Mexican origin.\n",
      "[00:15.360 --> 00:19.680]  One day I came over to her house and I saw she'd made this food for her family.\n",
      "[00:19.680 --> 00:20.600]  She was living with it.\n",
      "[00:20.600 --> 00:21.720]  They were loving the food.\n",
      "[00:22.440 --> 00:27.560]  As soon as I saw that, I thought, you know, that maybe this was something I could do.\n",
      "[00:27.560 --> 00:29.960]  I could bring Mexican-inspired food.\n",
      "✅ Successfully transcribed chunk_270_300.wav\n",
      "[00:00.000 --> 00:11.000]  cuisine to India.\n",
      "[00:11.000 --> 00:15.760]  When we were raising money for our first store, we estimated we needed about $100,000 to do\n",
      "[00:15.760 --> 00:16.760]  it.\n",
      "[00:16.760 --> 00:20.960]  We raised $250,000, so that was from a friend's family.\n",
      "[00:20.960 --> 00:26.720]  Two of my childhood friends decided to join, and we opened in October 2012, our first tour.\n",
      "[00:26.720 --> 00:27.920]  The store did extremely well.\n",
      "✅ Successfully transcribed chunk_300_330.wav\n",
      "[00:00.000 --> 00:04.400]  All of a sudden it was clear that we needed to grow our own ingredients.\n",
      "[00:04.400 --> 00:09.100]  So in 2018 we ordered Haas avocado trees from California.\n",
      "[00:09.100 --> 00:13.100]  We imported them and we planted them, about 500 of them.\n",
      "[00:13.100 --> 00:19.300]  Six months in some elephants came through the avocado farm and trampled 60 of the trees.\n",
      "[00:19.300 --> 00:25.000]  The past 13 years we've worked hard to make the food taste like you would get it in California,\n",
      "[00:25.000 --> 00:26.700]  or taste like you get it in Mexico.\n",
      "[00:26.700 --> 00:29.900]  If you use the tomatoes from India, they taste very different than the tomatoes.\n",
      "✅ Successfully transcribed chunk_330_360.wav\n",
      "[00:00.000 --> 00:04.280]  as you get in the U.S. And the onions taste very different, the beans taste different,\n",
      "[00:04.280 --> 00:11.400]  so it takes a lot of time to develop a supply chain that leads to a food that while is approachable\n",
      "[00:11.400 --> 00:17.400]  for someone who's from India is also differentiated from a flavor perspective.\n",
      "[00:17.400 --> 00:28.400]  So, when we break down that $23 million revenue, our food costs are about 37% food and packing\n",
      "[00:28.400 --> 00:29.400]  costs.\n",
      "✅ Successfully transcribed chunk_360_390.wav\n",
      "[00:00.000 --> 00:05.000]  loss would be about 12% and approximately 4% on marketing.\n",
      "[00:05.400 --> 00:10.400]  Real estate would be about 9% and then corporate overhead would be about 5% and\n",
      "[00:11.320 --> 00:13.920]  we have other operating expenses. So at the bottom line,\n",
      "[00:13.920 --> 00:15.000]  you could say it's around 10%.\n",
      "[00:16.280 --> 00:19.600]  Our plan is to reach 300 stores by 2030,\n",
      "[00:20.000 --> 00:25.000]  at which point we would probably list the company into an IPO in India and we'll\n",
      "[00:26.140 --> 00:26.980]  see what happens.\n",
      "✅ Successfully transcribed chunk_390_420.wav\n",
      "[00:00.000 --> 00:11.200]  For me, living in India feels like I'm a part of this incredible once-in-a-lifetime growth\n",
      "[00:11.200 --> 00:13.160]  story.\n",
      "[00:13.160 --> 00:19.520]  India is on a growth story that other countries have already gone through, and to be able\n",
      "[00:19.520 --> 00:25.380]  to witness the transformation and change that people's lives are experiencing is really\n",
      "[00:25.380 --> 00:26.440]  powerful.\n",
      "[00:26.440 --> 00:30.000]  I think it's also just a wonderful place to be from.\n",
      "✅ Successfully transcribed chunk_420_450.wav\n",
      "[00:00.000 --> 00:07.360]  life experience perspective and I couldn't be happier to be here.\n",
      "✅ Successfully transcribed chunk_450_470.5175625.wav\n",
      "✅ Full transcription complete! Saved as 'whisper_youtube_transcript.txt'.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "model_size = 'medium'  # Choose from ['tiny', 'base', 'small', 'medium', 'large']\n",
    "whisper_model = whisper.load_model(model_size)\n",
    "TARGET_LANGUAGE = \"en\"\n",
    "\n",
    "# 4. Transcribe chunks function\n",
    "def transcribe_chunks(chunks):\n",
    "    transcripts = []\n",
    "    for chunk_path in chunks:\n",
    "        try:\n",
    "            # Transcribe with Whisper\n",
    "            result = whisper_model.transcribe(\n",
    "                chunk_path,\n",
    "                language=TARGET_LANGUAGE,\n",
    "                temperature=0.15,\n",
    "                best_of=5,\n",
    "                beam_size=8,\n",
    "                fp16=False,  # Set to True if using GPU\n",
    "                verbose=True\n",
    "            )\n",
    "            transcripts.append(result[\"text\"])\n",
    "            print(f\"✅ Successfully transcribed {chunk_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❗ Error transcribing {chunk_path}: {e}\")\n",
    "            transcripts.append(\"\")\n",
    "    return transcripts\n",
    "\n",
    "# Transcribe all chunks\n",
    "whisper_transcripts = transcribe_chunks(chunks)\n",
    "\n",
    "# 5. Combine and save results\n",
    "final_whisper_text = ' '.join(whisper_transcripts)\n",
    "with open('whisper_youtube_transcript.txt', 'w') as f:\n",
    "    f.write(final_whisper_text)\n",
    "\n",
    "print(\"✅ Full transcription complete! Saved as 'whisper_youtube_transcript.txt'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
